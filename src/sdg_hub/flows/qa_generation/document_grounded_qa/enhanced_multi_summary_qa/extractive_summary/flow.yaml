metadata:
  name: Extractive Summary Knowledge Tuning Dataset Generation Flow
  description: Generate extractive summary from the input document. Each document is first converted into list of knowledge segments for creating extractive summary and then annotated with context, relationship and relevance. This is then converted
    into Question-Answer pairs.
  version: 2.0.0
  author: SDG Hub Contributors
  recommended_models:
    default: openai/gpt-oss-120b
    compatible:
    - meta-llama/Llama-3.3-70B-Instruct
    - microsoft/phi-4
    - mistralai/Mixtral-8x7B-Instruct-v0.1
    experimental: []
  tags:
  - knowledge-tuning
  - document-internalization
  - question-generation
  - knowledge-extractive-summary
  - qa-pairs
  - extractive-summaries
  license: Apache-2.0
  min_sdg_hub_version: 0.2.0
  dataset_requirements:
    required_columns:
    - document
    - document_outline
    - domain
    - icl_document
    - icl_query_1
    - icl_query_2
    - icl_query_3
    description: 'Input dataset should contain documents with text content and domain classification. Each document should be substantial enough for meaningful question generation (minimum 100 words recommended). The flow generates three types
      of summaries: detailed (n=20), extractive (n=10), and key facts (n=50), each producing corresponding QA pairs designed to help LLMs internalize document knowledge for knowledge tuning.'
  output_columns:
  - summary
  - question
  - response
  - raw_document
  - faithfulness_explanation
  - faithfulness_judgment
  id: epic-jade-656
blocks:
- block_type: DuplicateColumnsBlock
  block_config:
    block_name: duplicate_document_col
    input_cols:
      document: base_document
- block_type: PromptBuilderBlock
  block_config:
    block_name: extractive_summary_prompt
    input_cols:
    - document
    - document_outline
    output_cols: extractive_summary_prompt
    prompt_config_path: extractive_summary.yaml
    format_as_messages: true
- block_type: LLMChatBlock
  block_config:
    block_name: gen_extractive_summary
    input_cols: extractive_summary_prompt
    output_cols: raw_summary
    max_tokens: 4096
    temperature: 0.7
    n: 50
    async_mode: true
- block_type: TextParserBlock
  block_config:
    block_name: parse_extractive_summary
    input_cols: raw_summary
    output_cols: summary
    start_tags:
    - ''
    end_tags:
    - ''
- block_type: RenameColumnsBlock
  block_config:
    block_name: rename_to_document_column
    input_cols:
      document: raw_document
      summary: document
- block_type: PromptBuilderBlock
  block_config:
    block_name: question_generation_prompt
    input_cols:
    - domain
    - document
    - document_outline
    - icl_document
    - icl_query_1
    - icl_query_2
    - icl_query_3
    output_cols: question_generation_prompt
    prompt_config_path: ../generate_question_list.yaml
    format_as_messages: true
- block_type: LLMChatBlock
  block_config:
    block_name: question_generation
    input_cols: question_generation_prompt
    output_cols: question_list
    max_tokens: 256
    temperature: 0.7
    n: 1
    async_mode: true
- block_type: TextParserBlock
  block_config:
    block_name: parse_question_list
    input_cols: question_list
    output_cols: question
    start_tags:
    - '[QUESTION]'
    end_tags:
    - '[END]'
- block_type: PromptBuilderBlock
  block_config:
    block_name: answer_generation_prompt
    input_cols:
    - question
    - document
    - document_outline
    output_cols: answer_generation_prompt
    prompt_config_path: ../generate_answers.yaml
    format_as_messages: true
- block_type: LLMChatBlock
  block_config:
    block_name: answer_generation
    input_cols: answer_generation_prompt
    output_cols: response_dict
    max_tokens: 4096
    temperature: 0.7
    n: 1
    async_mode: true
- block_type: TextParserBlock
  block_config:
    block_name: parse_response_dict
    input_cols: response_dict
    output_cols: response
    start_tags:
    - ''
    end_tags:
    - ''
    save_reasoning_content: true
- block_type: EvaluateFaithfulnessBlock
  block_config:
    block_name: eval_faithfulness
    input_cols:
    - document
    - response
    output_cols:
    - faithfulness_explanation
    - faithfulness_judgment
    prompt_config_path: ../../multi_summary_qa/instructlab/evaluate_faithfulness.yaml
    filter_value: 'YES'
    operation: eq
    async_mode: true
    format_as_messages: true
    start_tags:
    - '[Start of Explanation]'
    - '[Start of Answer]'
    end_tags:
    - '[End of Explanation]'
    - '[End of Answer]'
