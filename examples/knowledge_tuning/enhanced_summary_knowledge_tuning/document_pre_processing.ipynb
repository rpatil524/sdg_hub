{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f458de",
   "metadata": {},
   "source": [
    "# Document Pre-processing for Knowledge Tuning\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete document preprocessing pipeline designed specifically for **knowledge tuning** with sdg-hub. \n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This preprocessing pipeline transforms raw documents (PDFs, Word docs, etc.) into seed data for data generation:\n",
    "\n",
    "1. **Document Parsing**: Converts raw documents to structured markdown format\n",
    "2. **Chunking**: Splits documents into manageable chunks while preserving structure and context\n",
    "3. **Seed Data Creation**: Formats chunks with in-context learning (ICL) templates for effective knowledge tuning\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- We will use the existing InstructLab document parser (`docparser_v2.py`) and Document parsing configuration (`docling_v2_config.yaml`)\n",
    "- Raw pdf documents in the `document_collection/` directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa22c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Document Processing Pipeline\n",
    "# Define the directory containing raw documents to be processed\n",
    "data_dir = \"document_collection/\"\n",
    "\n",
    "# Run the document parser to convert documents to markdown\n",
    "# - input-dir: Directory containing source documents\n",
    "# - output-dir: Directory where processed markdown files will be saved\n",
    "# - c: Configuration file specifying parsing parameters\n",
    "!python ../instructlab/docparser_v2.py --input-dir {data_dir} --output-dir {data_dir} -c ../instructlab/docling_v2_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295749b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install Required Dependencies\n",
    "# Install packages needed for document processing and text chunking\n",
    "\n",
    "%pip install docling markdown-it-py\n",
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Processed Document\n",
    "import glob\n",
    "\n",
    "# In our example above docling step produces markdown of all the pdf files in the document_collection\n",
    "with open(glob.glob(f\"{data_dir}/*.md\")[0], \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Text Chunking and Dataset Creation\n",
    "\n",
    "from markdown_it import MarkdownIt\n",
    "from typing import List\n",
    "import datasets\n",
    "\n",
    "\n",
    "def chunk_markdown(text: str, max_tokens: int = 200, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits Markdown text into chunks at block-level elements\n",
    "    (headings, paragraphs, lists, tables, code, blockquotes).\n",
    "    Adds overlap (in words) between all consecutive chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The markdown text to be chunked\n",
    "        max_tokens: Maximum number of words per chunk\n",
    "        overlap: Number of overlapping words between consecutive chunks\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks with specified overlap\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize markdown parser to understand document structure\n",
    "    md = MarkdownIt()\n",
    "    tokens = md.parse(text)\n",
    "\n",
    "    # Group tokens into block-level segments to preserve markdown structure\n",
    "    # This ensures we don't split in the middle of headings, lists, etc.\n",
    "    blocks = []\n",
    "    buf = []\n",
    "    for tok in tokens:\n",
    "        if tok.block and tok.type.endswith(\"_open\"):\n",
    "            buf = []\n",
    "        elif tok.block and tok.type.endswith(\"_close\"):\n",
    "            if buf:\n",
    "                blocks.append(\"\\n\".join(buf).strip())\n",
    "                buf = []\n",
    "        elif tok.content:\n",
    "            buf.append(tok.content)\n",
    "    if buf:\n",
    "        blocks.append(\"\\n\".join(buf).strip())\n",
    "\n",
    "    # Split blocks into chunks with overlap to maintain context continuity\n",
    "    chunks = []\n",
    "    current_words = []\n",
    "    for block in blocks:\n",
    "        words = block.split()\n",
    "        for w in words:\n",
    "            current_words.append(w)\n",
    "            if len(current_words) >= max_tokens:\n",
    "                # Emit a complete chunk\n",
    "                chunks.append(\" \".join(current_words))\n",
    "                # Prepare next buffer with overlap from the end of this chunk\n",
    "                # This ensures context continuity between chunks\n",
    "                current_words = current_words[-overlap:] if overlap > 0 else []\n",
    "\n",
    "    # Add any remaining words as the final chunk\n",
    "    if current_words:\n",
    "        chunks.append(\" \".join(current_words))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunks = chunk_markdown(text, max_tokens=5000, overlap=1000)\n",
    "\n",
    "\n",
    "# Prepare seed data for the SDG-Hub knowledge pipeline.\n",
    "#\n",
    "# The seed data requires the following fields:\n",
    "#   - document_outline: A concise title or summary that accurately represents the entire document.\n",
    "#     For documents covering multiple themes, consider providing multiple outlines (one per section).\n",
    "#   - icl_document: A representative sample extract from the document. This may include tables, code snippets, definitions, etc.\n",
    "#   - icl_query_1, icl_query_2, icl_query_3: Three questions based on the icl_document sample.\n",
    "#   - domain: The domain or subject area of the document.\n",
    "#\n",
    "# The code below creates a HuggingFace Dataset from the document chunks,\n",
    "# then maps the required ICL fields to each entry, and finally saves the result as a JSONL file.\n",
    "\n",
    "seed_data = datasets.Dataset.from_dict({\"document\": chunks})\n",
    "\n",
    "icl = {\n",
    "    \"document_outline\": \"The document contains excerpts from FINTRAC regulations designed to combat money laundering and terrorist financing in Canada\",\n",
    "    \"icl_document\": \"## Overview\\n\\nThis guidance came into effect on June 1, 2021.\\n\\n\\nThis guidance explains the methods that can be used by reporting entities\\n(REs) to verify the identity of a person or an entity.\\n\\n\\n## 1. Meaning of verifying the identity of a person or an entity\\n\\nIt means to use the methods described in this guidance to ensure that the\\ninformation in an identification document or from other informational\\nsources matches the information that the person or entity provided.\\n\\n\\nVerifying identity is a foundational element of Canada's anti-money\\nlaundering and anti-terrorist financing regime and a key component of an\\nRE's relationship with clients. It helps you to know your clients and to\\nunderstand and assess any risk that may be associated to their\\ntransactions or activities.\\n\\n\\n## 2. How to verify the identity of a person\\n\\nYou can use any of the 5 methods described below to identify a person:\\n\\n- 2.1 Government-issued photo identification method\\n\\n- 2.2 Credit file method\\n\\n- 2.3 Dual-process method\\n\\n- 2.4 Affiliate or member method\\n\\n- 2.5 Reliance method\\n\",\n",
    "    \"icl_query_1\": \"In Canada, what are the methods for verifying someone's identity?\",\n",
    "    \"icl_query_2\": \"In Canada, why is it important to confirm a client's identity?\",\n",
    "    \"icl_query_3\": \"In Canada, can I use Reliance method to verify identity of a person?\",\n",
    "    \"domain\": \"Finance\",\n",
    "}\n",
    "\n",
    "# Map the ICL fields to each document chunk (if you want to use the same ICL for all, as shown here)\n",
    "seed_data = seed_data.map(lambda x: icl)\n",
    "\n",
    "# Save the seed data to a JSONL file for downstream use\n",
    "seed_data.to_json(\"seed_data.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3ff7f",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "- The seed_data.jsonl file is now ready for the knowledge tuning pipeline.\n",
    "- You can now refer to the [knowledge generation](knowledge_generation.ipynb) notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdg_hub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
