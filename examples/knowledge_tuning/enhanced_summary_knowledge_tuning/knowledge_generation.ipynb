{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Summary Knowledge Tuning - Data Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to generate high-quality knowledge tuning datasets using the SDG Hub framework. It creates multiple types of document augmentations and corresponding question-answer pairs that can be used to train or fine-tune language models for enhanced summarization and knowledge extraction capabilities.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "2. **Generate Four Types of Knowledge Tuning Datasets**:\n",
    "   - **Extractive Summaries**: Concise summaries that extract key information directly from source documents\n",
    "   - **Detailed Summaries**: Comprehensive summaries that provide thorough coverage of document content\n",
    "   - **Key Facts**: Structured fact extraction with corresponding Q&A pairs\n",
    "   - **Document-Based Q&A**: Question-answer pairs generated directly from document content\n",
    "\n",
    "\n",
    "4. **Output Structured Training Data**:\n",
    "   - For each augmentation we save JSONL dataset.\n",
    "   - You can follow [knowledge_mixing](knowledge_mixing.ipynb) to convert it into training dataset\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- SDG Hub installed and configured\n",
    "- Environment variables set up (see [.env.example](.env.example)). Specifically set the model provider, seed data and output path.\n",
    "- Document pre-processing completed (run [document_pre_processing.ipynb](document_pre_processing.ipynb) first)\n",
    "\n",
    "```bash \n",
    "git clone https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub.git\n",
    "cd sdg_hub\n",
    "pip install .[examples]\n",
    "copy the .env.example to .env and set the model endpoint and generation/mixing parameters\n",
    "```\n",
    "**⚠️ If you haven't already, run the document pre-processing notebook to create the seed data.**\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After running this notebook, use [knowledge_mixing](knowledge_mixing.ipynb) to combine and curate the generated datasets for final model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# First Party\n",
    "from sdg_hub import Flow, FlowRegistry\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to run the flow with async mode\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seed_data_from_quality_benchmark(run_on_validation=None, seed_data_path=None):\n",
    "    \"\"\"\n",
    "    Create seed data from QuALITY Benchmark dataset.\n",
    "    \n",
    "    Args:\n",
    "        run_on_validation (bool, optional): If True, use validation subset. If None, reads from env.\n",
    "        seed_data_path (str, optional): Path to save seed data. If None, reads from env.\n",
    "    \n",
    "    Returns:\n",
    "        datasets.Dataset: The processed corpus\n",
    "    \"\"\"\n",
    "    # Use environment variables as defaults if not provided\n",
    "    if run_on_validation is None:\n",
    "        run_on_validation = os.getenv('RUN_ON_VALIDATION_SET', 'true').lower() == 'true'\n",
    "    if seed_data_path is None:\n",
    "        seed_data_path = os.getenv('SEED_DATA_PATH', 'seed_data_val.jsonl')\n",
    "    \n",
    "    # Load QuALITY Benchmark dataset\n",
    "    print(\"Loading QuALITY Benchmark dataset...\")\n",
    "    quality_corpus = load_dataset(\"zitongyang/entigraph-quality-corpus\", split='train').remove_columns(['entity', 'entigraph']).rename_columns({'raw': 'document', 'uid': 'document_outline'})\n",
    "    \n",
    "    # Define seed examples for knowledge tuning\n",
    "    seed_examples = {\n",
    "        \"icl_document\": (\n",
    "          \"The coastal town of Willow Creek, once renowned for its pristine beaches, now struggles with rampant pollution. Plastic debris and oil spills have devastated marine life, prompting a decline in tourism and fishing industries. Residents have organized weekly clean-up initiatives, but the scale of the problem overwhelms their efforts.\",\n",
    "          \"Technologists at the local university have developed an AI-powered buoy system to combat this. The buoys, equipped with solar panels and filtration technology, can identify and absorb oil spills while collecting microplastics. Data from the buoys is shared publicly, raising awareness and pressuring corporations to adopt sustainable practices. Though costly, the project has sparked hope for revitalizing the ecosystem and economy.\"\n",
    "        ),\n",
    "        \"icl_query_1\": \"How does the technological solution address the economic *and* environmental challenges highlighted in the document?\",\n",
    "        \"icl_query_2\": \"What implicit values or priorities do the community's actions (clean-up initiatives) and the technologists' project reflect, and how do these align or contrast?\",\n",
    "        \"icl_query_3\": \"Imagine the buoy project succeeds. What unintended consequences might arise from its impact, considering document's themes?\",\n",
    "        \"domain\": \"articles/essays\"\n",
    "    }\n",
    "    \n",
    "    # Add seed examples to the corpus\n",
    "    quality_corpus = quality_corpus.map(lambda x: seed_examples)\n",
    "    \n",
    "    if run_on_validation:\n",
    "        # Validation set - use predefined document IDs for consistent evaluation\n",
    "        DOC_UIDS = [\n",
    "            ' Defining Decay Down by David Plotz',\n",
    "            ' Fight Clubbed by David Plotz',\n",
    "            ' I, Antichrist? by Jeffrey Goldberg',\n",
    "            \" It's Time To Keelhaul U-Haul! by Jeffrey Goldberg\",\n",
    "            \" My Father's Estate by Ben Stein\",\n",
    "            '\"Phone Me in Central Park\" by McConnell, James V.',\n",
    "            'A Coffin for Jacob by Ludwig, Edward W.',\n",
    "            'A Fall of Glass by Lee, Stanley R.',\n",
    "            'A Filbert Is a Nut by Raphael, Rick',\n",
    "            'A Gift from Earth by Banister, Manly',\n",
    "            'A Gleeb for Earth by Schafhauser, Charles',\n",
    "            'A Good Year for the Roses? by David Edelstein',\n",
    "            'A Pail of Air by Leiber, Fritz',\n",
    "            'A Planet Named Joe by Hunter, Evan',\n",
    "            \"AI: what's the worst that could happen? by Harry Armstrong\",\n",
    "            'Accidental Death by Baily, Peter',\n",
    "            'All Day September by Kuykendall, Roger',\n",
    "            'Ambition by Bade, William L.',\n",
    "            'And Then the Town Took Off by Wilson, Richard',\n",
    "            'Atom Mystery [Young Atom Detective] by Coombs, Charles Ira',\n",
    "            'Beach Scene by King, Marshall',\n",
    "            'Big Ancestor by Wallace, F. L. (Floyd L.)',\n",
    "            'Birds of a Feather by Silverberg, Robert',\n",
    "            'Bodyguard by Gold, H. L. (Horace Leonard)'\n",
    "        ]\n",
    "        \n",
    "        # Filter corpus to validation set\n",
    "        quality_corpus = quality_corpus.filter(lambda x: x['document_outline'] in DOC_UIDS)\n",
    "        print(f\"Running on validation set with {len(quality_corpus)} documents\")\n",
    "    else:\n",
    "        # Use full dataset for training\n",
    "        print(f\"Running on full dataset with {len(quality_corpus)} documents\")\n",
    "    \n",
    "    # Save the seed data\n",
    "    quality_corpus.to_json(seed_data_path, orient='records', lines=True)\n",
    "    print(f\"Saved seed data to: {seed_data_path}\")\n",
    "    \n",
    "    return quality_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seed data. If one is not provided, create it from the quality benchmark dataset.\n",
    "seed_data_path = os.getenv('SEED_DATA_PATH', 'seed_data.jsonl')\n",
    "\n",
    "if not os.path.exists(seed_data_path):\n",
    "    print(f\"{seed_data_path} not found. Creating seed data...\")\n",
    "    quality_corpus = create_seed_data_from_quality_benchmark(seed_data_path=seed_data_path)\n",
    "else:\n",
    "    print(f\"Loading existing seed data from {seed_data_path}\")\n",
    "    quality_corpus = load_dataset('json', data_files=seed_data_path, split='train')\n",
    "\n",
    "# Subsample the seed data. Useful for debugging.\n",
    "subsample = int(os.getenv('SEED_DATA_SUBSAMPLE', '0'))\n",
    "if subsample > 0:\n",
    "    quality_corpus = quality_corpus.select(range(subsample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SDG\n",
    "- This will create knowledge flow from provided yaml file\n",
    "- We will run this on small dataset for demo purposes\n",
    "- For large scale generation, please use the python command provided in the next cell\n",
    "- You can analyze the generated data to ensure the quality is similar to proivded QnA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model configuration in flow object\n",
    "def set_model_config(flow_object):\n",
    "    model_provider = os.getenv('MODEL_PROVIDER', 'hosted_vllm')\n",
    "    print(f\"Using model provider: {model_provider}\")\n",
    "    # Set model provider\n",
    "    if model_provider == 'hosted_vllm':    \n",
    "        vllm_model = os.getenv('VLLM_MODEL', 'hosted_vllm/meta-llama/Llama-3.3-70B-Instruct')\n",
    "        vllm_api_base = os.getenv('VLLM_API_BASE', 'http://localhost:8000/v1')\n",
    "        vllm_api_key = os.getenv('VLLM_API_KEY', 'EMPTY')\n",
    "        enable_reasoning = os.getenv('ENABLE_REASONING', 'false').lower() in ('1', 'true', 'yes')\n",
    "        print(f\"Using reasoning: {enable_reasoning}\")\n",
    "        flow_object.set_model_config(\n",
    "            model=vllm_model,\n",
    "            api_base=vllm_api_base,\n",
    "            api_key=vllm_api_key,\n",
    "            enable_reasoning=enable_reasoning,\n",
    "        )\n",
    "    elif model_provider == 'openai':\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        openai_model = os.getenv('OPENAI_MODEL', 'openai/gpt-4')\n",
    "        flow_object.set_model_config(\n",
    "            model=openai_model,\n",
    "            api_key=openai_api_key,\n",
    "        )\n",
    "    elif model_provider == 'ollama':\n",
    "        ollama_model = os.getenv('OLLAMA_MODEL', 'ollama/gemma2')\n",
    "        ollama_api_base = os.getenv('OLLAMA_API_BASE', 'http://localhost:11434')\n",
    "        flow_object.set_model_config(\n",
    "            model=ollama_model,\n",
    "            api_base=ollama_api_base,\n",
    "        )\n",
    "    elif model_provider == 'maas':\n",
    "        maas_model = os.getenv('MAAS_MODEL')\n",
    "        maas_api_base = os.getenv('MAAS_API_BASE')\n",
    "        maas_api_key = os.getenv('MAAS_API_KEY')\n",
    "        flow_object.set_model_config(\n",
    "            model=maas_model,\n",
    "            api_base=maas_api_base,\n",
    "            api_key=maas_api_key,\n",
    "        )\n",
    "    return flow_object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover the available generation flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-discover all available flows (no setup needed!)\n",
    "FlowRegistry.discover_flows()\n",
    "\n",
    "# List available flows\n",
    "flows = FlowRegistry.list_flows()\n",
    "print(f\"Available flows: {flows}\")\n",
    "\n",
    "# You can also search the flows by tag\n",
    "qa_flows = FlowRegistry.search_flows(tag=\"question-generation\")\n",
    "print(f\"QA flows: {qa_flows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get runtime parameters\n",
    "enable_reasoning = os.getenv('ENABLE_REASONING', 'false').lower() in ('1', 'true', 'yes')\n",
    "number_of_summaries = int(os.getenv('NUMBER_OF_SUMMARIES', '50'))\n",
    "max_concurrency = int(os.getenv('MAX_CONCURRENCY', '50'))\n",
    "save_data_path = os.getenv('OUTPUT_DATA_FOLDER', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for extractive summary\n",
    "flow_name = \"Extractive Summary Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow = set_model_config(flow)\n",
    "number_of_summaries = int(os.getenv('NUMBER_OF_SUMMARIES', '50'))\n",
    "# Generate data for extractive summary\n",
    "if enable_reasoning:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        'question_generation': {'max_tokens': 1024}, \n",
    "        'gen_extractive_summary': {'n': number_of_summaries, 'max_tokens': 6000}\n",
    "        }\n",
    "else:\n",
    "    runtime_params = {\n",
    "    'gen_extractive_summary': {\n",
    "        'n': number_of_summaries\n",
    "    }\n",
    "}\n",
    "\n",
    "extractive_summary_generated_data = flow.generate(quality_corpus, runtime_params=runtime_params, max_concurrency=max_concurrency)\n",
    "\n",
    "extractive_summary_generated_data.to_json(os.path.join(save_data_path, 'extractive_summary', 'gen.jsonl'), orient='records', lines=True)\n",
    "\n",
    "print(f\"✓ Extractive summary: {len(extractive_summary_generated_data)} records\")\n",
    "\n",
    "print(f\"✓ Columns: {list(extractive_summary_generated_data.column_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate similar data for Detailed Summary\n",
    "flow_name = \"Detailed Summary Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow = set_model_config(flow)\n",
    "\n",
    "if enable_reasoning:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        'question_generation': {'max_tokens': 1024}, \n",
    "        'gen_detailed_summary': {'n': number_of_summaries, 'max_tokens': 6000}\n",
    "        }\n",
    "else:\n",
    "    runtime_params = ({'gen_detailed_summary': {\n",
    "        'n': number_of_summaries\n",
    "    }})\n",
    "# Generate data for detailed summary\n",
    "detailed_summary_generated_data = flow.generate(quality_corpus, runtime_params=runtime_params, max_concurrency=50)\n",
    "\n",
    "detailed_summary_generated_data.to_json(os.path.join(save_data_path, 'detailed_summary', 'gen.jsonl'), orient='records', lines=True)\n",
    "\n",
    "print(f\"✓ Detailed summary: {len(detailed_summary_generated_data)} records\")\n",
    "\n",
    "print(f\"✓ Columns: {list(detailed_summary_generated_data.column_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate similar data for key facts \n",
    "flow_name = \"Key Facts Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow = set_model_config(flow)\n",
    "runtime_params = {}\n",
    "if enable_reasoning:\n",
    "    # Increase max tokens for Question Generation to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        'generate_key_fact_qa': {'max_tokens': 6000}, \n",
    "        }\n",
    "\n",
    "# Generate data for key facts summary\n",
    "key_facts_generated_data = flow.generate(quality_corpus, runtime_params=runtime_params, max_concurrency=max_concurrency)\n",
    "\n",
    "key_facts_generated_data.to_json(os.path.join(save_data_path, 'key_facts_to_qa', 'gen.jsonl'), orient='records', lines=True)\n",
    "\n",
    "print(f\"✓ Key facts: {len(key_facts_generated_data)} records\")\n",
    "\n",
    "print(f\"✓ Columns: {list(key_facts_generated_data.column_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_name = \"Document Based Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow = set_model_config(flow)\n",
    "runtime_params = {}\n",
    "if enable_reasoning:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        'question_generation': {'max_tokens': 2048}, \n",
    "        }\n",
    "\n",
    "document_based_generated_data = flow.generate(quality_corpus, runtime_params=runtime_params, max_concurrency=max_concurrency)\n",
    "    \n",
    "document_based_generated_data.to_json(os.path.join(save_data_path, 'document_based_qa', 'gen.jsonl'), orient='records', lines=True)\n",
    "\n",
    "print(f\"✓ Document based: {len(document_based_generated_data)} records\")\n",
    "\n",
    "print(f\"✓ Columns: {list(document_based_generated_data.column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 You now have all three four of document augmentations (detailed summaries, extractive summaries, key facts and document based) along with their corresponding QA pairs.\n",
    "\n",
    "✅ Next steps:\n",
    "   - Combine and curate these datasets to prepare your final training data.\n",
    "   - For detailed guidance on post-processing, mixing, and formatting the data for model training (including conversion to messages format), please refer to [knowledge_mixing.ipynb](knowledge_mixing.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdg_hub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
