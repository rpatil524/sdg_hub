{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Text Insights Extraction Demo\n",
    "\n",
    "This notebook demonstrates the **Structured Text Insights Flow** using the Bloomberg Financial News dataset. \n",
    "\n",
    "## What You'll Learn\n",
    "- How to use the structured insights flow for comprehensive text analysis\n",
    "- Extract summaries, keywords, entities, and sentiment from financial news\n",
    "- Analyze and visualize results across large datasets\n",
    "- Extend the flow with custom blocks for domain-specific analysis\n",
    "\n",
    "## Flow Capabilities\n",
    "The structured insights flow performs **4 key analyses** on any text:\n",
    "1. **üìù Summary**: Concise 2-3 sentence summaries\n",
    "2. **üîë Keywords**: Top 10 most important terms\n",
    "3. **üè∑Ô∏è Entities**: Named entities (people, organizations, locations)\n",
    "4. **üòä Sentiment**: Emotional tone analysis (positive/negative/neutral)\n",
    "\n",
    "All results are combined into a **structured JSON output** for easy processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# pip install sdg_hub[examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from datasets import load_dataset\n",
    "import nest_asyncio\n",
    "\n",
    "from sdg_hub import Flow, FlowRegistry\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Required for async execution in notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Flow Discovery and Loading\n",
    "\n",
    "SDG Hub automatically discovers all available flows. Let's find our structured insights flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-discover all available flows\n",
    "FlowRegistry.discover_flows()\n",
    "\n",
    "# List all flows\n",
    "flows = FlowRegistry.list_flows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for text analysis flows\n",
    "text_flows = FlowRegistry.search_flows(tag=\"text-analysis\")\n",
    "print(f\"Text analysis flows: {text_flows}\")\n",
    "\n",
    "# Load our structured insights flow\n",
    "flow_id = \"green-clay-812\" \n",
    "flow_path = FlowRegistry.get_flow_path(flow_id)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded flow: {flow_id}\") \n",
    "\n",
    "flow.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "The flow supports multiple LLM models. Let's configure it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recommended models\n",
    "print(\"Default model:\", flow.get_default_model())\n",
    "print(\"Model recommendations:\", flow.get_model_recommendations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the flow to use a specific model\n",
    "# Option 1: Use a local vLLM server\n",
    "flow.set_model_config(\n",
    "    model=\"hosted_vllm/openai/gpt-oss-20b\",\n",
    "    api_base=\"http://localhost:8201/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    # this only works with models which support reasoning_effort\n",
    "    # if your model does not support it, you can remove this parameter\n",
    "    extra_body={\"reasoning_effort\": \"low\"}\n",
    ")\n",
    "\n",
    "# Option 2: Use OpenAI (requires API key)\n",
    "# flow.set_model_config(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     api_key=\"your-openai-api-key\"\n",
    "# )\n",
    "\n",
    "# Option 3: Use Anthropic Claude (requires API key)\n",
    "# flow.set_model_config(\n",
    "#     model=\"anthropic/claude-3-haiku\",\n",
    "#     api_key=\"your-anthropic-api-key\"\n",
    "# )\n",
    "\n",
    "print(\"‚úÖ Model configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Exploration\n",
    "\n",
    "We'll use the **Bloomberg Financial News dataset** - 447k financial news articles from 2006-2013:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Bloomberg Financial News dataset\n",
    "print(\"Loading Bloomberg Financial News dataset...\")\n",
    "dataset = load_dataset(\"danidanou/Bloomberg_Financial_News\", split=\"train\")\n",
    "\n",
    "print(f\"üìä Dataset size: {len(dataset):,} articles\")\n",
    "print(f\"üìÖ Columns: {dataset.column_names}\")\n",
    "print(f\"üíæ Dataset features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "sample = dataset[0]\n",
    "print(\"=== Sample Article ===\")\n",
    "print(f\"Headline: {sample['Headline']}\")\n",
    "print(f\"Date: {sample['Date']}\")\n",
    "print(f\"Journalists: {sample['Journalists']}\")\n",
    "print(f\"Article length: {len(sample['Article'])} characters\")\n",
    "print(f\"Article preview: {sample['Article'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small sample for demonstration (start with 50 articles)\n",
    "# For production, you can process thousands of articles\n",
    "sample_size = 50\n",
    "demo_dataset = dataset.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "print(f\"üìù Demo dataset prepared: {len(demo_dataset)} articles\")\n",
    "print(f\"üìä Average article length: {sum(len(article['Article']) for article in demo_dataset) / len(demo_dataset):.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover what dataset schema is expected by the flow\n",
    "\n",
    "schema_dataset = flow.get_dataset_schema() \n",
    "print(f\"Required columns: {schema_dataset.column_names}\")\n",
    "print(f\"Schema: {schema_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The flow expects a 'text' column, so we'll use rename the 'Article' column to 'text'\n",
    "demo_dataset = demo_dataset.rename_column(\"Article\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Structured Insights Flow\n",
    "\n",
    "Now let's extract structured insights from our financial news articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate structured insights\n",
    "print(\"üöÄ Running structured insights extraction...\")\n",
    "print(\"‚è±Ô∏è This may take a few minutes depending on your model setup...\")\n",
    "\n",
    "# Run the flow\n",
    "results = flow.generate(demo_dataset)\n",
    "\n",
    "print(\"‚úÖ Processing complete!\")\n",
    "print(f\"üìä Generated insights for {len(results)} articles\")\n",
    "print(f\"üìã Result columns: {results.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample result\n",
    "sample_result = results[random.randint(0, len(results) - 1)]\n",
    "\n",
    "print(\"=== First Article Analysis ===\")\n",
    "print(f\"üì∞ Original headline: {dataset[0]['Headline']}\")\n",
    "print(f\"üìÖ Date: {dataset[0]['Date']}\")\n",
    "print(f\"‚úçÔ∏è Journalists: {dataset[0]['Journalists']}\")\n",
    "print(f\"üìÑ Article length: {len(sample_result['text'])} characters\")\n",
    "print()\n",
    "\n",
    "# Parse and display the structured insights\n",
    "insights = json.loads(sample_result[\"structured_insights\"])\n",
    "print(\"üîç EXTRACTED INSIGHTS:\")\n",
    "print(json.dumps(insights, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dynamic Flow Extension: Adding Stock Ticker Extraction\n",
    "\n",
    "Now we'll demonstrate SDG Hub's **dynamic flow modification** capabilities. Instead of creating separate flow files, we can extend flows at runtime by adding custom processing blocks using existing SDG Hub components.\n",
    "\n",
    "### What We'll Add:\n",
    "We'll extend our structured insights flow to extract **stock ticker symbols** from financial news articles. This is perfect for Bloomberg financial news analysis!\n",
    "\n",
    "### Approach:\n",
    "We'll use three existing SDG Hub blocks:\n",
    "1. **PromptBuilderBlock** - Create a prompt to extract stock tickers\n",
    "2. **LLMChatBlock** - Process the extraction using the LLM\n",
    "3. **TextParserBlock** - Parse the output to a clean list\n",
    "\n",
    "Let's see how to modify flows at runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll modify the existing flow by adding our ticker extraction blocks\n",
    "# First, let's examine the current flow structure\n",
    "flow.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the blocks we need\n",
    "from sdg_hub.core.blocks.llm import LLMChatBlock, PromptBuilderBlock, TextParserBlock, LLMParserBlock\n",
    "from sdg_hub.core.blocks.transform import JSONStructureBlock\n",
    "\n",
    "# Step 1: Add stock ticker extraction blocks to the flow\n",
    "print(\"üöÄ Adding stock ticker extraction blocks to the flow...\")\n",
    "\n",
    "# Create the stock ticker extraction blocks\n",
    "ticker_prompt_block = PromptBuilderBlock(\n",
    "    block_name=\"stock_ticker_prompt\",\n",
    "    input_cols=[\"text\"],\n",
    "    output_cols=[\"ticker_prompt\"],\n",
    "    prompt_config_path=\"extract_stock_tickers.yaml\"\n",
    ")\n",
    "\n",
    "ticker_llm_block = LLMChatBlock(\n",
    "    block_name=\"extract_stock_tickers\",\n",
    "    input_cols=[\"ticker_prompt\"],\n",
    "    output_cols=[\"raw_stock_tickers\"],\n",
    "    max_tokens=512,\n",
    "    temperature=0.1  # Low temperature for more consistent extraction\n",
    ")\n",
    "\n",
    "ticker_llm_parser_block = LLMParserBlock(\n",
    "    block_name=\"extract_stock_tickers\",\n",
    "    input_cols=[\"raw_stock_tickers\"],\n",
    "    extract_content=True,\n",
    "    expand_lists=True\n",
    ")\n",
    "\n",
    "ticker_parser_block = TextParserBlock(\n",
    "    block_name=\"parse_stock_tickers\",\n",
    "    input_cols=[\"extract_stock_tickers_content\"],\n",
    "    output_cols=[\"stock_tickers\"],\n",
    "    start_tags=[\"[STOCK_TICKERS]\"],\n",
    "    end_tags=[\"[/STOCK_TICKERS]\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created ticker extraction blocks:\")\n",
    "print(f\"  1. {ticker_prompt_block.block_name} - Builds extraction prompt\")\n",
    "print(f\"  2. {ticker_llm_block.block_name} - Extracts tickers via LLM\")\n",
    "print(f\"  3. {ticker_parser_block.block_name} - Parses LLM output\")\n",
    "\n",
    "# Step 2: Update the JSONStructureBlock to include stock tickers\n",
    "print(\"üîß Updating JSON structure to include stock ticker field...\")\n",
    "\n",
    "# Create a new JSONStructureBlock configuration that includes our new stock_tickers field\n",
    "enhanced_json_block = JSONStructureBlock(\n",
    "    block_name=\"create_enhanced_structured_insights\",\n",
    "    input_cols=[\"summary\", \"keywords\", \"entities\", \"sentiment\", \"stock_tickers\"],\n",
    "    output_cols=[\"enhanced_structured_insights\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enhanced JSON structure will include:\")\n",
    "print(\"  üìù summary - Article summary\")\n",
    "print(\"  üîë keywords - Important keywords\")\n",
    "print(\"  üè∑Ô∏è entities - Named entities\")\n",
    "print(\"  üòä sentiment - Emotional tone\")\n",
    "print(\"  üìà stock_tickers - Stock ticker symbols (NEW!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove the original JSONStructureBlock (if it exists in your flow/blocks list)\n",
    "# (Assume we are not using a flow object here, just not using the old block.)\n",
    "\n",
    "# Add the new blocks to a list for the enhanced pipeline\n",
    "ticker_blocks = [\n",
    "    ticker_prompt_block,\n",
    "    ticker_llm_block,\n",
    "    ticker_llm_parser_block,\n",
    "    ticker_parser_block,\n",
    "    enhanced_json_block\n",
    "]\n",
    "\n",
    "flow.blocks.pop()\n",
    "flow.blocks.extend(ticker_blocks)\n",
    "flow.print_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the new LLM blocks with our model settings\n",
    "flow.set_model_config(\n",
    "    model=\"hosted_vllm/openai/gpt-oss-20b\",\n",
    "    api_base=\"http://localhost:8201/v1\", \n",
    "    api_key=\"EMPTY\",\n",
    "    # this only works with models which support reasoning_effort\n",
    "    # if your model does not support it, you can remove this parameter\n",
    "    extra_body={\"reasoning_effort\": \"low\"}\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Ready to run enhanced flow with stock ticker extraction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate structured insights\n",
    "print(\"üöÄ Running structured insights extraction...\")\n",
    "print(\"‚è±Ô∏è This may take a few minutes depending on your model setup...\")\n",
    "\n",
    "# Run the flow\n",
    "results2 = flow.generate(demo_dataset)\n",
    "\n",
    "print(\"‚úÖ Processing complete!\")\n",
    "print(f\"üìä Generated insights for {len(results2)} articles\")\n",
    "print(f\"üìã Result columns: {results2.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample result\n",
    "sample_result2 = results2[random.randint(0, len(results2) - 1)]\n",
    "\n",
    "print(\"=== First Article Analysis ===\")\n",
    "print(f\"üì∞ Original headline: {dataset[0]['Headline']}\")\n",
    "print(f\"üìÖ Date: {dataset[0]['Date']}\")\n",
    "print(f\"‚úçÔ∏è Journalists: {dataset[0]['Journalists']}\")\n",
    "print(f\"üìÑ Article length: {len(sample_result2['text'])} characters\")\n",
    "print()\n",
    "\n",
    "# Parse and display the structured insights\n",
    "insights2 = json.loads(sample_result2[\"enhanced_structured_insights\"])\n",
    "print(\"üîç EXTRACTED INSIGHTS:\")\n",
    "print(json.dumps(insights2, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### üß™ **Experiment Further**\n",
    "1. **Scale up**: Process 100+ articles to see larger patterns\n",
    "2. **Time analysis**: Filter by date ranges to see trends over time\n",
    "3. **Model comparison**: Try different LLMs and compare results\n",
    "4. **Custom prompts**: Modify the prompt templates for your domain\n",
    "\n",
    "### üîß **Customize for Your Use Case**\n",
    "1. **Domain adaptation**: Modify prompts for your specific industry\n",
    "2. **Additional insights**: Add blocks for topic classification, urgency scoring, etc.\n",
    "3. **Output format**: Customize JSON structure for your applications\n",
    "4. **Quality filters**: Add validation and quality checks\n",
    "\n",
    "### üöÄ Build Your Own Model\n",
    "- Leverage the generated structured insights as high-quality training data for your own machine learning models.\n",
    "- Fine-tune LLMs or train classifiers to automate similar analyses at scale.\n",
    "- Refer to Training Hub (https://github.com/Red-Hat-AI-Innovation-Team/training_hub) to setup your own training pipeline.\n",
    "\n",
    "### üìö **Learn More**\n",
    "- Explore other SDG Hub flows in the repository\n",
    "- Check the documentation for advanced configuration options\n",
    "- Join the community for questions and contributions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
